# RLG Engine - Retrieval-Locked Generation

> **Better than RAG. Totally Offline. Near-Zero Hallucination.**

RLG is a grounded question-answering system that goes beyond traditional RAG (Retrieval-Augmented Generation) by enforcing citation verification at every step.

## ğŸ¯ Key Differences from Standard RAG

| Standard RAG | RLG Engine |
|--------------|------------|
| Vector similarity only | **Multi-stage retrieval** (BM25 + Dense + Structural) |
| Generate then hope | **Generate then validate** |
| Trust LLM output | **Verify every sentence against sources** |
| Cloud API dependency | **100% offline** with Ollama |
| Black-box answers | **Citation links for every claim** |

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.10+**
2. **Ollama** (for local LLM)
   ```bash
   # Windows (via installer)
   # Download from https://ollama.ai
   
   # Start Ollama
   ollama serve
   
   # Pull a model (choose one)
   ollama pull mistral    # 7B, balanced
   ollama pull llama3     # 8B, high quality
   ollama pull phi3       # 3.8B, fast
   ```

### Installation

```bash
cd rlg-backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Download embedding model (runs once)
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
```

### Run the Server

```bash
uvicorn app.main:app --reload --port 8000
```

Visit: http://localhost:8000/docs

## ğŸ“ Project Structure

```
rlg-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # All settings
â”‚   â”‚   â””â”€â”€ database.py      # SQLite + FTS5
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ document.py      # Document metadata
â”‚   â”‚   â”œâ”€â”€ chunk.py         # Text chunks with structure
â”‚   â”‚   â”œâ”€â”€ query.py         # Query tracking
â”‚   â”‚   â””â”€â”€ answer.py        # Grounded answers
â”‚   â”œâ”€â”€ schemas/             # Pydantic models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ embedding_service.py    # Local embeddings
â”‚   â”‚   â”œâ”€â”€ ingestion_service.py    # Document processing
â”‚   â”‚   â”œâ”€â”€ vector_index_service.py # FAISS vector search
â”‚   â”‚   â”œâ”€â”€ retrieval_service.py    # Multi-stage retrieval
â”‚   â”‚   â”œâ”€â”€ context_service.py      # Context building
â”‚   â”‚   â”œâ”€â”€ llm_service.py          # Ollama integration
â”‚   â”‚   â””â”€â”€ validation_service.py   # Grounding verification
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ documents.py     # Document CRUD
â”‚   â”‚   â””â”€â”€ query.py         # Q&A endpoints
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ tokenizer.py     # Text processing
â”‚       â””â”€â”€ scoring.py       # Ranking metrics
â”œâ”€â”€ data/                    # Local data storage
â”‚   â”œâ”€â”€ uploads/             # Uploaded documents
â”‚   â”œâ”€â”€ indices/             # FAISS indices
â”‚   â””â”€â”€ cache/               # Model cache
â””â”€â”€ requirements.txt
```

## ğŸ”§ API Endpoints

### Documents

```bash
# Upload a document
curl -X POST "http://localhost:8000/documents/upload" \
  -F "file=@my_document.pdf" \
  -F "category=technical" \
  -F "reliability_score=0.9"

# List documents
curl "http://localhost:8000/documents/"

# Get document chunks
curl "http://localhost:8000/documents/{id}/chunks"
```

### Query

```bash
# Ask a question (with grounding validation)
curl -X POST "http://localhost:8000/query/" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the main topic?", "require_grounding": true}'

# Extractive mode (direct quotes only)
curl -X POST "http://localhost:8000/query/extractive" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the key features?"}'

# Streaming response
curl -N "http://localhost:8000/query/stream" \
  -H "Content-Type: application/json" \
  -d '{"question": "Explain the architecture"}'
```

## âš™ï¸ Configuration

Edit `.env` or `app/core/config.py`:

```env
# LLM Settings
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=mistral

# Retrieval Weights (must sum to 1.0)
BM25_WEIGHT=0.3
DENSE_WEIGHT=0.5
STRUCTURAL_WEIGHT=0.2

# Grounding Threshold
MIN_GROUNDING_CONFIDENCE=0.7
REQUIRE_EXACT_CITATION=true
```

## ğŸ§ª How It Works

### 1. Multi-Stage Retrieval

```
Query â†’ BM25 (keyword) â”€â”
      â†’ Dense (semantic) â”œâ†’ Merge â†’ Structural Rerank â†’ Top-K
      â†’ Structural â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Context Building

```
Top-K Chunks â†’ Add Citation Markers [1], [2]...
             â†’ Build Grounded Prompt
             â†’ Include Citation Key
```

### 3. Grounding Validation

For each sentence in the LLM response:
1. Check citation markers exist
2. Verify cited content matches source
3. Compute semantic similarity to sources
4. Mark as grounded/ungrounded

### 4. Response Filtering

```python
if grounding_score < MIN_THRESHOLD:
    return "No grounded answer found"
else:
    return answer_with_citations
```

## ğŸ“Š Grounding Score

Every response includes a grounding score (0-1):

- **1.0**: Every sentence verified against sources
- **0.7+**: High confidence, some inferences
- **0.5-0.7**: Moderate grounding
- **<0.5**: Rejected by default

## ğŸ”’ Offline Guarantee

Everything runs locally:
- **Embeddings**: sentence-transformers (downloads model once)
- **Vector Search**: FAISS (local index)
- **Database**: SQLite with FTS5
- **LLM**: Ollama (local)

No data leaves your machine.

## ğŸ› ï¸ Extending RLG

### Add Custom Document Types

```python
# In ingestion_service.py
def _process_custom(self, document, filepath):
    # Your custom processing
    return chunks
```

### Add Vector Index

Switch from FAISS to other options:
- ChromaDB
- Qdrant
- Milvus

### Add UI Layer

The API is designed for any frontend:
- React/Next.js
- Streamlit
- Gradio

## ğŸ“ License

MIT License - Use freely, attribute kindly.

---

Built for **grounded truth**, not creative fiction. ğŸ¯
